{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on April 13 (with 2-hour grace period) and worth 25 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1. You will:\n",
    "\n",
    "1. Evaluate baseline models using default settings.\n",
    "2. Engineer new features and re-evaluate models.\n",
    "3. Use feature selection techniques to find promising subsets.\n",
    "4. Select the top 3 models and fine-tune them for optimal performance.\n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling.\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">   df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will not use the testing set during this milestone — it’s reserved for final evaluation later.\n",
    "- You will have to redo the scaling step when you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many code cells as you need\n",
    "df_clean = pd.read_csv('zillow_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airconditioningtypeid</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>finishedsquarefeet12</th>\n",
       "      <th>fips</th>\n",
       "      <th>fullbathcnt</th>\n",
       "      <th>garagecarcnt</th>\n",
       "      <th>...</th>\n",
       "      <th>regionidcity</th>\n",
       "      <th>regionidcounty</th>\n",
       "      <th>regionidneighborhood</th>\n",
       "      <th>regionidzip</th>\n",
       "      <th>roomcnt</th>\n",
       "      <th>unitcnt</th>\n",
       "      <th>yearbuilt</th>\n",
       "      <th>numberofstories</th>\n",
       "      <th>censustractandblock</th>\n",
       "      <th>taxvaluedollarcnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>6059.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53571.0</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>118849.0</td>\n",
       "      <td>96978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.059063e+13</td>\n",
       "      <td>1023282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13091.0</td>\n",
       "      <td>2061.0</td>\n",
       "      <td>118849.0</td>\n",
       "      <td>97099.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.111001e+13</td>\n",
       "      <td>464000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>6059.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21412.0</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>118849.0</td>\n",
       "      <td>97078.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.059022e+13</td>\n",
       "      <td>564778.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>396551.0</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>118849.0</td>\n",
       "      <td>96330.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.037300e+13</td>\n",
       "      <td>145143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12447.0</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>268548.0</td>\n",
       "      <td>96451.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.037124e+13</td>\n",
       "      <td>119407.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   airconditioningtypeid  bathroomcnt  bedroomcnt  buildingqualitytypeid  \\\n",
       "0                    1.0          3.5         4.0                    6.0   \n",
       "1                    1.0          1.0         2.0                    6.0   \n",
       "2                    1.0          2.0         3.0                    6.0   \n",
       "3                    1.0          3.0         4.0                    8.0   \n",
       "4                    1.0          3.0         3.0                    8.0   \n",
       "\n",
       "   calculatedbathnbr  calculatedfinishedsquarefeet  finishedsquarefeet12  \\\n",
       "0                3.5                        3100.0                3100.0   \n",
       "1                1.0                        1465.0                1465.0   \n",
       "2                2.0                        1243.0                1243.0   \n",
       "3                3.0                        2376.0                2376.0   \n",
       "4                3.0                        1312.0                1312.0   \n",
       "\n",
       "     fips  fullbathcnt  garagecarcnt  ...  regionidcity  regionidcounty  \\\n",
       "0  6059.0          3.0           2.0  ...       53571.0          1286.0   \n",
       "1  6111.0          1.0           1.0  ...       13091.0          2061.0   \n",
       "2  6059.0          2.0           2.0  ...       21412.0          1286.0   \n",
       "3  6037.0          3.0           2.0  ...      396551.0          3101.0   \n",
       "4  6037.0          3.0           2.0  ...       12447.0          3101.0   \n",
       "\n",
       "   regionidneighborhood  regionidzip  roomcnt  unitcnt  yearbuilt  \\\n",
       "0              118849.0      96978.0      0.0      1.0     1998.0   \n",
       "1              118849.0      97099.0      5.0      1.0     1967.0   \n",
       "2              118849.0      97078.0      6.0      1.0     1962.0   \n",
       "3              118849.0      96330.0      0.0      1.0     1970.0   \n",
       "4              268548.0      96451.0      0.0      1.0     1964.0   \n",
       "\n",
       "   numberofstories  censustractandblock  taxvaluedollarcnt  \n",
       "0              1.0         6.059063e+13          1023282.0  \n",
       "1              1.0         6.111001e+13           464000.0  \n",
       "2              1.0         6.059022e+13           564778.0  \n",
       "3              1.0         6.037300e+13           145143.0  \n",
       "4              1.0         6.037124e+13           119407.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test sets\n",
    "X = df_clean.drop(columns=['taxvaluedollarcnt'])\n",
    "y = df_clean['taxvaluedollarcnt']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Baseline Modeling [3 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters**:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each model:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV RMSE Score** across all folds in a table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n"
     ]
    }
   ],
   "source": [
    "#Linear regression using repeated cross-validation 5 folds and 5 repeats\n",
    "lr = LinearRegression()\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)  \n",
    "scores_lr = cross_val_score(lr, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_lr = np.sqrt(-scores_lr)\n",
    "rmse_mean_lr = rmse_scores_lr.mean()\n",
    "rmse_std_lr = rmse_scores_lr.std()\n",
    "rmse_table = pd.DataFrame({'RMSE Mean': [rmse_mean_lr], 'RMSE Std': [rmse_std_lr]})\n",
    "rmse_table.index = ['Linear Regression']\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n"
     ]
    }
   ],
   "source": [
    "#Ridge regression using repeated cross-validation 5 folds and 5 repeats\n",
    "ridge = Ridge(random_state=random_state)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_rr = cross_val_score(ridge, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_rr = np.sqrt(-scores_rr )\n",
    "rmse_mean_rr  = rmse_scores_rr .mean()\n",
    "rmse_std_rr  = rmse_scores_rr .std()\n",
    "rmse_table.loc['Ridge'] = [rmse_mean_rr , rmse_std_rr ]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "#Lasso regression using repeated cross-validation 5 folds and 5 repeats\n",
    "# Suppress ConvergenceWarnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    lasso = Lasso(random_state=random_state)\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "    scores_l = cross_val_score(lasso, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_l = np.sqrt(-scores_l)\n",
    "rmse_mean_l = rmse_scores_l.mean()\n",
    "rmse_std_l = rmse_scores_l.std()\n",
    "rmse_table.loc['Lasso'] = [rmse_mean_l, rmse_std_l]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n",
      "ElasticNet         294075.484151   4496.946761\n"
     ]
    }
   ],
   "source": [
    "#ElasticNet regression using repeated cross-validation 5 folds and 5 repeats\n",
    "elastic_net = ElasticNet(random_state=random_state)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_en = cross_val_score(elastic_net, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_en = np.sqrt(-scores_en)\n",
    "rmse_mean_en = rmse_scores_en.mean()\n",
    "rmse_std_en = rmse_scores_en.std()\n",
    "rmse_table.loc['ElasticNet'] = [rmse_mean_en, rmse_std_en]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n",
      "ElasticNet         294075.484151   4496.946761\n",
      "Decision Tree      344708.754694   5576.475372\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Regressor using repeated cross-validation 5 folds and 5 repeats\n",
    "dt = DecisionTreeRegressor(random_state=random_state)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_dt = cross_val_score(dt, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_dt = np.sqrt(-scores_dt)\n",
    "rmse_mean_dt = rmse_scores_dt.mean()\n",
    "rmse_std_dt = rmse_scores_dt.std()\n",
    "rmse_table.loc['Decision Tree'] = [rmse_mean_dt, rmse_std_dt]\n",
    "print(rmse_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n",
      "ElasticNet         294075.484151   4496.946761\n",
      "Decision Tree      344708.754694   5576.475372\n",
      "Bagging            259849.854440   4266.661459\n"
     ]
    }
   ],
   "source": [
    "#Bagging Regressor using repeated cross-validation 5 folds and 5 repeats\n",
    "bagging = BaggingRegressor(random_state=random_state)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_bagging = cross_val_score(bagging, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_bagging = np.sqrt(-scores_bagging)\n",
    "rmse_mean_bagging = rmse_scores_bagging.mean()\n",
    "rmse_std_bagging = rmse_scores_bagging.std()\n",
    "rmse_table.loc['Bagging'] = [rmse_mean_bagging, rmse_std_bagging]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n",
      "ElasticNet         294075.484151   4496.946761\n",
      "Decision Tree      344708.754694   5576.475372\n",
      "Bagging            259849.854440   4266.661459\n",
      "Random Forest      249410.093806   3926.383553\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Regressor using repeated cross-validation 5 folds and 5 repeats\n",
    "rf = RandomForestRegressor(random_state=random_state, n_jobs=-1)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_rf = cross_val_score(rf, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_rf = np.sqrt(-scores_rf)\n",
    "rmse_mean_rf = rmse_scores_rf.mean()\n",
    "rmse_std_rf = rmse_scores_rf.std()\n",
    "rmse_table.loc['Random Forest'] = [rmse_mean_rf, rmse_std_rf]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n",
      "ElasticNet         294075.484151   4496.946761\n",
      "Decision Tree      344708.754694   5576.475372\n",
      "Bagging            259849.854440   4266.661459\n",
      "Random Forest      249410.093806   3926.383553\n",
      "Gradient Boosting  252881.425307   3908.142996\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting Regressor using repeated cross-validation 5 folds and 5 repeats\n",
    "gb = GradientBoostingRegressor(random_state=random_state)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_gb = cross_val_score(gb, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_gb = np.sqrt(-scores_gb)\n",
    "rmse_mean_gb = rmse_scores_gb.mean()\n",
    "rmse_std_gb = rmse_scores_gb.std()\n",
    "rmse_table.loc['Gradient Boosting'] = [rmse_mean_gb, rmse_std_gb]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [2 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which models perform best overall?\n",
    "  - Which are most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here\n",
    "\n",
    "### **Which models perform best overall?**\n",
    "The RMSE (Root Mean Square Error) Mean represents the average error magnitude, so lower values indicate better performance. **Random Forest** has the lowest RMSE Mean (249,410.09), making it the best-performing model overall. Close behind is **Gradient Boosting**, with a slightly higher RMSE Mean (252,881.43). Both models seem to balance predictive accuracy quite well.\n",
    "\n",
    "### **Which models are most stable (lowest Std)?**\n",
    "The RMSE Std (Standard Deviation) reflects consistency. A lower Std suggests the model is stable across predictions. **Gradient Boosting** takes the lead here, with the lowest RMSE Std (3,908.14), followed by **Random Forest** (3,926.38). These two models are the most stable.\n",
    "\n",
    "### **Signs of overfitting or underfitting?**\n",
    "- **Linear Regression** shows high RMSE Mean (296,756.90) and Std (44,392.55), indicating potential **underfitting**, as it may be too simple to capture the complexities of the data.\n",
    "- **Decision Tree** has the highest RMSE Mean (344,708.75) and a very low RMSE Std (5,576.48). These characteristics strongly suggest **overfitting**, as the model might be excessively tailored to the training data at the cost of generalization.\n",
    "- **Bagging**, **Random Forest**, and **Gradient Boosting** exhibit relatively low RMSE Mean and Std. They strike a good balance and likely have minimal issues with overfitting or underfitting.\n",
    "\n",
    "### Summary:\n",
    "For overall performance and stability, **Random Forest** and **Gradient Boosting** are your top choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [3 pts]\n",
    "\n",
    "Consider **at least three new features** based on your Milestone 1, Part 5. Examples include:\n",
    "- Polynomial terms\n",
    "- Log or interaction terms\n",
    "- Groupings or transformations of categorical features\n",
    "\n",
    "Add these features to `X_train` and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run all models listed above (using default settings again).\n",
    "- Report updated RMSE scores (mean and std) across repeated CV in a table. \n",
    "\n",
    "**Note:**  Recall that this will require creating a new version of the dataset, so effectively you may be running \"polynomial regression\" using `LinearRegression`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many code cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [2 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "- Were there any unexpected results?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [3 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features.\n",
    "- Report updated RMSE scores (mean and std) across repeated CV in a table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [2 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n",
    "\n",
    "- How did feature selection differ between linear and tree-based models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Top 3 Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far.\n",
    "\n",
    "1. Choose the top 3 models based on performance and interpretability from earlier parts.\n",
    "2. For each model:\n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, or other techniques from previous homeworks. \n",
    "   - Experiment with different versions of your feature engineering and preprocessing — treat these as additional tunable components.\n",
    "3. Report the mean and standard deviation of CV RMSE score for each model in a summary table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many code cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [4 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n",
    "- Provide a ranking of your three models and explain your reasoning — not just based on RMSE, but also interpretability, training time, or generalizability.\n",
    "- Conclude by considering whether this workflow has produced the results you expected. Typically, you would repeat steps 2 - 4 and also reconsider the choices you made in Milestone 1 when cleaning the dataset, until reaching the point of diminishing returns; do you think that would that have helped here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
