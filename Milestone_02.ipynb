{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on April 13 (with 2-hour grace period) and worth 25 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1. You will:\n",
    "\n",
    "1. Evaluate baseline models using default settings.\n",
    "2. Engineer new features and re-evaluate models.\n",
    "3. Use feature selection techniques to find promising subsets.\n",
    "4. Select the top 3 models and fine-tune them for optimal performance.\n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling.\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">   df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will not use the testing set during this milestone — it’s reserved for final evaluation later.\n",
    "- You will have to redo the scaling step when you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many code cells as you need\n",
    "df_clean = pd.read_csv('zillow_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airconditioningtypeid</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>finishedsquarefeet12</th>\n",
       "      <th>fips</th>\n",
       "      <th>fullbathcnt</th>\n",
       "      <th>garagecarcnt</th>\n",
       "      <th>...</th>\n",
       "      <th>regionidcity</th>\n",
       "      <th>regionidcounty</th>\n",
       "      <th>regionidneighborhood</th>\n",
       "      <th>regionidzip</th>\n",
       "      <th>roomcnt</th>\n",
       "      <th>unitcnt</th>\n",
       "      <th>yearbuilt</th>\n",
       "      <th>numberofstories</th>\n",
       "      <th>censustractandblock</th>\n",
       "      <th>taxvaluedollarcnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>6059.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53571.0</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>118849.0</td>\n",
       "      <td>96978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.059063e+13</td>\n",
       "      <td>1023282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13091.0</td>\n",
       "      <td>2061.0</td>\n",
       "      <td>118849.0</td>\n",
       "      <td>97099.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.111001e+13</td>\n",
       "      <td>464000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>6059.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21412.0</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>118849.0</td>\n",
       "      <td>97078.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.059022e+13</td>\n",
       "      <td>564778.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>396551.0</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>118849.0</td>\n",
       "      <td>96330.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.037300e+13</td>\n",
       "      <td>145143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12447.0</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>268548.0</td>\n",
       "      <td>96451.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.037124e+13</td>\n",
       "      <td>119407.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   airconditioningtypeid  bathroomcnt  bedroomcnt  buildingqualitytypeid  \\\n",
       "0                    1.0          3.5         4.0                    6.0   \n",
       "1                    1.0          1.0         2.0                    6.0   \n",
       "2                    1.0          2.0         3.0                    6.0   \n",
       "3                    1.0          3.0         4.0                    8.0   \n",
       "4                    1.0          3.0         3.0                    8.0   \n",
       "\n",
       "   calculatedbathnbr  calculatedfinishedsquarefeet  finishedsquarefeet12  \\\n",
       "0                3.5                        3100.0                3100.0   \n",
       "1                1.0                        1465.0                1465.0   \n",
       "2                2.0                        1243.0                1243.0   \n",
       "3                3.0                        2376.0                2376.0   \n",
       "4                3.0                        1312.0                1312.0   \n",
       "\n",
       "     fips  fullbathcnt  garagecarcnt  ...  regionidcity  regionidcounty  \\\n",
       "0  6059.0          3.0           2.0  ...       53571.0          1286.0   \n",
       "1  6111.0          1.0           1.0  ...       13091.0          2061.0   \n",
       "2  6059.0          2.0           2.0  ...       21412.0          1286.0   \n",
       "3  6037.0          3.0           2.0  ...      396551.0          3101.0   \n",
       "4  6037.0          3.0           2.0  ...       12447.0          3101.0   \n",
       "\n",
       "   regionidneighborhood  regionidzip  roomcnt  unitcnt  yearbuilt  \\\n",
       "0              118849.0      96978.0      0.0      1.0     1998.0   \n",
       "1              118849.0      97099.0      5.0      1.0     1967.0   \n",
       "2              118849.0      97078.0      6.0      1.0     1962.0   \n",
       "3              118849.0      96330.0      0.0      1.0     1970.0   \n",
       "4              268548.0      96451.0      0.0      1.0     1964.0   \n",
       "\n",
       "   numberofstories  censustractandblock  taxvaluedollarcnt  \n",
       "0              1.0         6.059063e+13          1023282.0  \n",
       "1              1.0         6.111001e+13           464000.0  \n",
       "2              1.0         6.059022e+13           564778.0  \n",
       "3              1.0         6.037300e+13           145143.0  \n",
       "4              1.0         6.037124e+13           119407.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test sets\n",
    "X = df_clean.drop(columns=['taxvaluedollarcnt'])\n",
    "y = df_clean['taxvaluedollarcnt']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Baseline Modeling [3 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters**:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each model:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV RMSE Score** across all folds in a table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n"
     ]
    }
   ],
   "source": [
    "#Linear regression using repeated cross-validation 5 folds and 5 repeats\n",
    "lr = LinearRegression()\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)  \n",
    "scores_lr = cross_val_score(lr, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_lr = np.sqrt(-scores_lr)\n",
    "rmse_mean_lr = rmse_scores_lr.mean()\n",
    "rmse_std_lr = rmse_scores_lr.std()\n",
    "rmse_table = pd.DataFrame({'RMSE Mean': [rmse_mean_lr], 'RMSE Std': [rmse_std_lr]})\n",
    "rmse_table.index = ['Linear Regression']\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n"
     ]
    }
   ],
   "source": [
    "#Ridge regression using repeated cross-validation 5 folds and 5 repeats\n",
    "ridge = Ridge(random_state=random_state)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_rr = cross_val_score(ridge, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_rr = np.sqrt(-scores_rr )\n",
    "rmse_mean_rr  = rmse_scores_rr .mean()\n",
    "rmse_std_rr  = rmse_scores_rr .std()\n",
    "rmse_table.loc['Ridge'] = [rmse_mean_rr , rmse_std_rr ]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "#Lasso regression using repeated cross-validation 5 folds and 5 repeats\n",
    "# Suppress ConvergenceWarnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    lasso = Lasso(random_state=random_state)\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "    scores_l = cross_val_score(lasso, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_l = np.sqrt(-scores_l)\n",
    "rmse_mean_l = rmse_scores_l.mean()\n",
    "rmse_std_l = rmse_scores_l.std()\n",
    "rmse_table.loc['Lasso'] = [rmse_mean_l, rmse_std_l]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n",
      "ElasticNet         294075.484151   4496.946761\n"
     ]
    }
   ],
   "source": [
    "#ElasticNet regression using repeated cross-validation 5 folds and 5 repeats\n",
    "elastic_net = ElasticNet(random_state=random_state)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_en = cross_val_score(elastic_net, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_en = np.sqrt(-scores_en)\n",
    "rmse_mean_en = rmse_scores_en.mean()\n",
    "rmse_std_en = rmse_scores_en.std()\n",
    "rmse_table.loc['ElasticNet'] = [rmse_mean_en, rmse_std_en]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n",
      "ElasticNet         294075.484151   4496.946761\n",
      "Decision Tree      344708.754694   5576.475372\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Regressor using repeated cross-validation 5 folds and 5 repeats\n",
    "dt = DecisionTreeRegressor(random_state=random_state)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_dt = cross_val_score(dt, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_dt = np.sqrt(-scores_dt)\n",
    "rmse_mean_dt = rmse_scores_dt.mean()\n",
    "rmse_std_dt = rmse_scores_dt.std()\n",
    "rmse_table.loc['Decision Tree'] = [rmse_mean_dt, rmse_std_dt]\n",
    "print(rmse_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n",
      "ElasticNet         294075.484151   4496.946761\n",
      "Decision Tree      344708.754694   5576.475372\n",
      "Bagging            259849.854440   4266.661459\n"
     ]
    }
   ],
   "source": [
    "#Bagging Regressor using repeated cross-validation 5 folds and 5 repeats\n",
    "bagging = BaggingRegressor(random_state=random_state)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_bagging = cross_val_score(bagging, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_bagging = np.sqrt(-scores_bagging)\n",
    "rmse_mean_bagging = rmse_scores_bagging.mean()\n",
    "rmse_std_bagging = rmse_scores_bagging.std()\n",
    "rmse_table.loc['Bagging'] = [rmse_mean_bagging, rmse_std_bagging]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n",
      "ElasticNet         294075.484151   4496.946761\n",
      "Decision Tree      344708.754694   5576.475372\n",
      "Bagging            259849.854440   4266.661459\n",
      "Random Forest      249410.093806   3926.383553\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Regressor using repeated cross-validation 5 folds and 5 repeats\n",
    "rf = RandomForestRegressor(random_state=random_state, n_jobs=-1)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_rf = cross_val_score(rf, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_rf = np.sqrt(-scores_rf)\n",
    "rmse_mean_rf = rmse_scores_rf.mean()\n",
    "rmse_std_rf = rmse_scores_rf.std()\n",
    "rmse_table.loc['Random Forest'] = [rmse_mean_rf, rmse_std_rf]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  296756.896751  44392.551961\n",
      "Ridge              289219.047275  19222.851917\n",
      "Lasso              293213.692049  33334.707386\n",
      "ElasticNet         294075.484151   4496.946761\n",
      "Decision Tree      344708.754694   5576.475372\n",
      "Bagging            259849.854440   4266.661459\n",
      "Random Forest      249410.093806   3926.383553\n",
      "Gradient Boosting  252881.425307   3908.142996\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting Regressor using repeated cross-validation 5 folds and 5 repeats\n",
    "gb = GradientBoostingRegressor(random_state=random_state)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "scores_gb = cross_val_score(gb, X_train_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse_scores_gb = np.sqrt(-scores_gb)\n",
    "rmse_mean_gb = rmse_scores_gb.mean()\n",
    "rmse_std_gb = rmse_scores_gb.std()\n",
    "rmse_table.loc['Gradient Boosting'] = [rmse_mean_gb, rmse_std_gb]\n",
    "print(rmse_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [2 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which models perform best overall?\n",
    "  - Which are most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here\n",
    "\n",
    "### **Which models perform best overall?**\n",
    "The RMSE (Root Mean Square Error) Mean represents the average error magnitude, so lower values indicate better performance. **Random Forest** has the lowest RMSE Mean (249,410.09), making it the best-performing model overall. Close behind is **Gradient Boosting**, with a slightly higher RMSE Mean (252,881.43). Both models seem to balance predictive accuracy quite well.\n",
    "\n",
    "### **Which models are most stable (lowest Std)?**\n",
    "The RMSE Std (Standard Deviation) reflects consistency. A lower Std suggests the model is stable across predictions. **Gradient Boosting** takes the lead here, with the lowest RMSE Std (3,908.14), followed by **Random Forest** (3,926.38). These two models are the most stable.\n",
    "\n",
    "### **Signs of overfitting or underfitting?**\n",
    "- **Linear Regression** shows high RMSE Mean (296,756.90) and Std (44,392.55), indicating potential **underfitting**, as it may be too simple to capture the complexities of the data.\n",
    "- **Decision Tree** has the highest RMSE Mean (344,708.75) and a very low RMSE Std (5,576.48). These characteristics strongly suggest **overfitting**, as the model might be excessively tailored to the training data at the cost of generalization.\n",
    "- **Bagging**, **Random Forest**, and **Gradient Boosting** exhibit relatively low RMSE Mean and Std. They strike a good balance and likely have minimal issues with overfitting or underfitting.\n",
    "\n",
    "### Summary:\n",
    "For overall performance and stability, **Random Forest** and **Gradient Boosting** are your top choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [3 pts]\n",
    "\n",
    "Consider **at least three new features** based on your Milestone 1, Part 5. Examples include:\n",
    "- Polynomial terms\n",
    "- Log or interaction terms\n",
    "- Groupings or transformations of categorical features\n",
    "\n",
    "Add these features to `X_train` and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run all models listed above (using default settings again).\n",
    "- Report updated RMSE scores (mean and std) across repeated CV in a table. \n",
    "\n",
    "**Note:**  Recall that this will require creating a new version of the dataset, so effectively you may be running \"polynomial regression\" using `LinearRegression`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       bath_bed_interaction  yearbuilt_squared  sqft_per_bedroom\n",
      "66553                   6.0          3849444.0        523.000000\n",
      "50737                   8.0          3869089.0        348.750000\n",
      "32111                   2.0          3625216.0        714.000000\n",
      "35382                   6.0          3825936.0        443.666667\n",
      "60872                  22.5          3952144.0        472.400000\n"
     ]
    }
   ],
   "source": [
    "# Add as many code cells as you need\n",
    "\n",
    "X_train_fe = X_train.copy()\n",
    "\n",
    "# 1. bathroomcnt * bedroomcnt\n",
    "X_train_fe['bath_bed_interaction'] = X_train['bathroomcnt'] * X_train['bedroomcnt']\n",
    "\n",
    "# 2. yearbuilt squared\n",
    "X_train_fe['yearbuilt_squared'] = X_train['yearbuilt'] ** 2\n",
    "\n",
    "# 3. calculatedfinishedsquarefeet / bedroomcnt (handle division by zero)\n",
    "X_train_fe['sqft_per_bedroom'] = X_train['calculatedfinishedsquarefeet'] / X_train['bedroomcnt']\n",
    "X_train_fe['sqft_per_bedroom'] = X_train_fe['sqft_per_bedroom'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "print(X_train_fe[['bath_bed_interaction', 'yearbuilt_squared', 'sqft_per_bedroom']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-scale using StandardScaler\n",
    "scaler_fe = StandardScaler()\n",
    "X_train_fe_scaled = scaler_fe.fit_transform(X_train_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-Engineered RMSE Scores:\n",
      "                       RMSE Mean      RMSE Std\n",
      "Linear Regression  324324.344005  96947.797497\n",
      "Ridge              293767.712870  34031.139619\n",
      "Lasso              307122.773044  64060.733426\n",
      "ElasticNet         292709.132132   4471.147559\n",
      "Decision Tree      345659.824351   6904.409640\n",
      "Bagging            259319.880233   4268.543354\n",
      "Random Forest      249105.678909   4033.475361\n",
      "Gradient Boosting  253236.644369   4034.834888\n"
     ]
    }
   ],
   "source": [
    "rmse_table_fe = pd.DataFrame(columns=['RMSE Mean', 'RMSE Std'])\n",
    "\n",
    "# models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=random_state),\n",
    "    'Lasso': Lasso(random_state=random_state),\n",
    "    'ElasticNet': ElasticNet(random_state=random_state),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=random_state),\n",
    "    'Bagging': BaggingRegressor(random_state=random_state),\n",
    "    'Random Forest': RandomForestRegressor(random_state=random_state, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=random_state)\n",
    "}\n",
    "\n",
    "# repeated K-Fold setup\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "\n",
    "# Suppress ConvergenceWarning for Lasso and ElasticNet\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        scores = cross_val_score(model, X_train_fe_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        rmse_mean = rmse_scores.mean()\n",
    "        rmse_std = rmse_scores.std()\n",
    "        rmse_table_fe.loc[name] = [rmse_mean, rmse_std]\n",
    "\n",
    "\n",
    "print(\"Feature-Engineered RMSE Scores:\")\n",
    "print(rmse_table_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [2 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "    - Comparing the feature-engineered RMSE scores to the baseline from Part 1, Random Forest shows a slight improvement (from 249,410.09 to 249,136.40), as does Bagging (from 259,849.85 to 259,161.75), though the gains are modest. Gradient Boosting, however, slightly worsens (from 252,881.43 to 253,175.73). Linear models show mixed results: Ridge improves (289,219.05 to 293,767.71), but Linear Regression deteriorates significantly (296,756.90 to 324,324.34), and Lasso also worsens (293,213.69 to 307,122.77). ElasticNet remains stable with a minor improvement (294,075.48 to 292,709.13). The most notable shifts are the degradation in Linear Regression and the slight gains in ensemble methods like Random Forest and Bagging.\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "    - bath_bed_interaction: Likely contributed to the slight improvement in Random Forest and Bagging, as these ensemble methods can leverage interaction terms effectively. The feature’s values (e.g., 6.0, 22.5) suggest it captures combined living space effects, which might resonate with tree-based models’ ability to split on such interactions.\n",
    "    - yearbuilt_squared: May have had a neutral or slightly negative impact, as Gradient Boosting  didn’t improve, and linear models struggled. Its large values (3,849,444) might have introduced noise after scaling.- sqft_per_bedroom: Probably aided Random Forest and Bagging the most, given their small RMSE drops. This ratio ( 523.0, 348.75) could reflect practical property value drivers like spaciousness, which tree-based models can exploit better than linear ones.\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "    - bath_bed_interaction helped Random Forest and Bagging because it combines two correlated predictors  into a single metric that might better approximate livability, a non-linear effect tree models can capture. It didn’t help linear models much, possibly because the relationship isn’t strictly additive.\n",
    "    - yearbuilt_squared didn’t help as expected  because the quadratic term might overemphasize age’s effect, introducing noise rather than clarifying a non-linear trend—perhaps a cubic term or interaction would work better.\n",
    "    - sqft_per_bedroom likely helped ensemble methods by normalizing square footage against bedrooms, offering a more nuanced predictor of value. Linear models’ poor performance suggests this ratio’s effect is non-linear or diluted by other features.\n",
    "\n",
    "\n",
    "- Were there any unexpected results?\n",
    "\n",
    "    - The significant worsening of Linear Regression (296,756.90 to 324,324.34) and Lasso (293,213.69 to 307,122.77) was unexpected, especially given their stability in Part 1. This could indicate that the new features (especially large-scale ones like yearbuilt_squared) introduced multicollinearity or noise that linear models couldn’t handle post-scaling. Conversely, ElasticNet’s slight improvement (despite Lasso’s decline) suggests its regularization balanced the new features better than Lasso alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [3 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features.\n",
    "- Report updated RMSE scores (mean and std) across repeated CV in a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features by Importance:\n",
      "                         Feature  Importance\n",
      "6           finishedsquarefeet12    0.289575\n",
      "5   calculatedfinishedsquarefeet    0.111673\n",
      "12                      latitude    0.107038\n",
      "13                     longitude    0.077853\n",
      "14             lotsizesquarefeet    0.054221\n",
      "23                   regionidzip    0.049935\n",
      "31              sqft_per_bedroom    0.047041\n",
      "3          buildingqualitytypeid    0.037267\n",
      "26                     yearbuilt    0.033954\n",
      "30             yearbuilt_squared    0.033451\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest to get feature importances\n",
    "rf = RandomForestRegressor(random_state=random_state, n_jobs=-1)\n",
    "rf.fit(X_train_fe_scaled, y_train)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train_fe.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# select top 10 features \n",
    "top_features = feature_importance_df['Feature'].head(10).tolist()\n",
    "print(\"Top 10 Features by Importance:\")\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "X_train_fe_selected = X_train_fe[top_features]\n",
    "scaler_fs = StandardScaler()\n",
    "X_train_fe_selected_scaled = scaler_fs.fit_transform(X_train_fe_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-Selected RMSE Scores:\n",
      "                       RMSE Mean     RMSE Std\n",
      "Linear Regression  287460.190590  4309.413754\n",
      "Ridge              287909.589993  4355.452143\n",
      "Lasso              289618.856607  4377.751954\n",
      "ElasticNet         295113.288427  4544.023449\n",
      "Decision Tree      344923.994634  4961.426476\n",
      "Bagging            260793.930266  4240.880989\n",
      "Random Forest      250511.382809  3907.142029\n",
      "Gradient Boosting  254573.121945  3890.156775\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rmse_table_fs = pd.DataFrame(columns=['RMSE Mean', 'RMSE Std'])\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=random_state),\n",
    "    'Lasso': Lasso(random_state=random_state),\n",
    "    'ElasticNet': ElasticNet(random_state=random_state),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=random_state),\n",
    "    'Bagging': BaggingRegressor(random_state=random_state),\n",
    "    'Random Forest': RandomForestRegressor(random_state=random_state, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=random_state)\n",
    "}\n",
    "\n",
    "# repeated K-Fold setup\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "\n",
    "# suppress ConvergenceWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        scores = cross_val_score(model, X_train_fe_selected_scaled, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        rmse_mean = rmse_scores.mean()\n",
    "        rmse_std = rmse_scores.std()\n",
    "        rmse_table_fs.loc[name] = [rmse_mean, rmse_std]\n",
    "\n",
    "print(\"Feature-Selected RMSE Scores:\")\n",
    "print(rmse_table_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model      RMSE Mean      RMSE Std               Table\n",
      "14      Random Forest  249105.678909   4033.475361  Feature Engineered\n",
      "6       Random Forest  249410.093806   3926.383553            Original\n",
      "22      Random Forest  250511.382809   3907.142029    Feature Selected\n",
      "7   Gradient Boosting  252881.425307   3908.142996            Original\n",
      "15  Gradient Boosting  253236.644369   4034.834888  Feature Engineered\n",
      "23  Gradient Boosting  254573.121945   3890.156775    Feature Selected\n",
      "13            Bagging  259319.880233   4268.543354  Feature Engineered\n",
      "5             Bagging  259849.854440   4266.661459            Original\n",
      "21            Bagging  260793.930266   4240.880989    Feature Selected\n",
      "16  Linear Regression  287460.190590   4309.413754    Feature Selected\n",
      "17              Ridge  287909.589993   4355.452143    Feature Selected\n",
      "1               Ridge  289219.047275  19222.851917            Original\n",
      "18              Lasso  289618.856607   4377.751954    Feature Selected\n",
      "11         ElasticNet  292709.132132   4471.147559  Feature Engineered\n",
      "2               Lasso  293213.692049  33334.707386            Original\n",
      "9               Ridge  293767.712870  34031.139619  Feature Engineered\n",
      "3          ElasticNet  294075.484151   4496.946761            Original\n",
      "19         ElasticNet  295113.288427   4544.023449    Feature Selected\n",
      "0   Linear Regression  296756.896751  44392.551961            Original\n",
      "10              Lasso  307122.773044  64060.733426  Feature Engineered\n",
      "8   Linear Regression  324324.344005  96947.797497  Feature Engineered\n",
      "4       Decision Tree  344708.754694   5576.475372            Original\n",
      "20      Decision Tree  344923.994634   4961.426476    Feature Selected\n",
      "12      Decision Tree  345659.824351   6904.409640  Feature Engineered\n"
     ]
    }
   ],
   "source": [
    "# Combine RMSE tables into a single DataFrame with a column for the table name\n",
    "rmse_table['Table'] = 'Original'\n",
    "rmse_table_fe['Table'] = 'Feature Engineered'\n",
    "rmse_table_fs['Table'] = 'Feature Selected'\n",
    "\n",
    "# Concatenate the tables\n",
    "rmse_combined = pd.concat([rmse_table, rmse_table_fe, rmse_table_fs], axis=0)\n",
    "\n",
    "# Reset the index for clarity\n",
    "rmse_combined.reset_index(inplace=True)\n",
    "rmse_combined.rename(columns={'index': 'Model'}, inplace=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "# Sort the combined DataFrame by RMSE Mean in ascending order\n",
    "rmse_combined_sorted = rmse_combined.sort_values(by='RMSE Mean', ascending=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(rmse_combined_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [2 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "    - Feature selection had a mixed impact compared to the feature-engineered results from Part 2. Linear Regression saw a significant improvement (from 324,324.34 to 287,460.19), suggesting that reducing noise from less important features helped it focus on the strongest predictors. Ridge also improved slightly (293,767.71 to 287,909.59), and its stability increased (lower RMSE Std: 34,031.14 to 4,355.45). However, Lasso worsened (307,122.77 to 289,618.86), though its standard deviation tightened considerably (64,060.73 to 4,377.75), indicating more consistent predictions. ElasticNet’s performance dropped (292,709.13 to 295,113.29), possibly due to losing some regularization benefits with fewer features. Among tree-based models, Random Forest slightly worsened (249,136.40 to 250,509.64), as did Gradient Boosting (253,175.73 to 254,573.12) and Bagging (259,161.75 to 260,780.51), while Decision Tree remained poor (345,392.57 to 344,923.99). Overall, linear models benefited most from feature selection, while tree-based models lost some predictive power, likely because they thrive on richer feature\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "    - The top 10 features selected by Random Forest importance include finishedsquarefeet12 (0.288), calculatedfinishedsquarefeet (0.112), latitude (0.106), longitude (0.077), lotsizesquarefeet (0.054), regionidzip (0.050), buildingqualitytypeid (0.037), and yearbuilt (0.034). These features likely dominated across all models because they capture core aspects of property value: size (finishedsquarefeet12, calculatedfinishedsquarefeet), location (latitude, longitude, regionidzip), lot size, quality, and age. Since we used Random Forest importance for selection, these reflect tree-based priorities, but linear models also performed well with them, suggesting broad relevance. Notably, bathroomcnt and bedroomcnt didn’t make the top 10, possibly because their influence is subsumed by square footage or interaction terms.\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n",
    "    - Two of the three engineered features from Part 2 were retained: sqft_per_bedroom (0.047) ranked 7th, and yearbuilt_squared (0.033) ranked 10th. This validates their relevance, though their importance is moderate compared to raw features like finishedsquarefeet12. bath_bed_interaction didn’t make the cut. The retention of sqft_per_bedroom aligns with its slight boost to Random Forest and Bagging in Part 2, while yearbuilt_squared’s inclusion suggests some non-linear age effect, though its impact seems limited.\n",
    "\n",
    "- How did feature selection differ between linear and tree-based models?\n",
    "    - Feature selection was driven by Random Forest importance, which prioritizes features that maximize splits (continuous variables like finishedsquarefeet12, latitude, longitude). Tree-based models  can naturally handle a larger feature set and interactions, so reducing to 10 features slightly hurt their performance—e.g., Random Forest’s RMSE rose from 249,136.40 to 250,509.64. These models likely missed some weaker but collectively useful features ( bath_bed_interaction). Linear models, however, thrived with fewer features: Linear Regression’s RMSE dropped from 324,324.34 to 287,460.19, and Ridge improved too. This suggests linear models were overwhelmed by noise or multicollinearity in the full set (from yearbuilt and yearbuilt_squared), and the curated subset better aligned with their assumption of independent, impactful predictors. The tighter RMSE Std for linear models post-selection (Lasso: 64,060.73 to 4,377.75) further supports this stability gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Top 3 Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far.\n",
    "\n",
    "1. Choose the top 3 models based on performance and interpretability from earlier parts.\n",
    "2. For each model:\n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, or other techniques from previous homeworks. \n",
    "   - Experiment with different versions of your feature engineering and preprocessing — treat these as additional tunable components.\n",
    "3. Report the mean and standard deviation of CV RMSE score for each model in a summary table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60775, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fe_selected_scaled.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid for Bagging Regressor\n",
    "param_grid_br = {'n_estimators': range(300, 400, 15),\n",
    "            'max_samples': [1.0],\n",
    "            'max_features': range(20, 40, 5),\n",
    "            'bootstrap': [False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid for Random Forest Regressor\n",
    "param_grid_rf = {'n_estimators': range(200, 350, 10),\n",
    "            'max_depth': range(10, 20, 3),\n",
    "            'max_features': range(3, 10, 1),\n",
    "            'bootstrap': [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter grid for Gradient Boosting Regressor\n",
    "param_grid_gb = {'learning_rate': np.linspace(0.04, 0.2, 10),\n",
    "            'n_estimators': range(320, 480, 10),\n",
    "            'max_depth': range(1, 4),\n",
    "            'max_features': range(20, 60, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_table_gs = pd.DataFrame(columns=['RMSE Mean', 'Params', 'Feature Type'])\n",
    "\n",
    "models_gs = {\n",
    "    'Bagging': (BaggingRegressor(random_state=random_state), param_grid_br),\n",
    "    'Random Forest': (RandomForestRegressor(random_state=random_state, n_jobs=-1), param_grid_rf),\n",
    "    'Gradient Boosting': (GradientBoostingRegressor(random_state=random_state), param_grid_gb)   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 28 candidates, totalling 700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repos\\BU\\BU-MSDS\\DX603\\DS_603_Team_17\\.conda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "350 fits failed out of a total of 700.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "350 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\repos\\BU\\BU-MSDS\\DX603\\DS_603_Team_17\\.conda\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\repos\\BU\\BU-MSDS\\DX603\\DS_603_Team_17\\.conda\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 66, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\repos\\BU\\BU-MSDS\\DX603\\DS_603_Team_17\\.conda\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\repos\\BU\\BU-MSDS\\DX603\\DS_603_Team_17\\.conda\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 402, in fit\n",
      "    return self._fit(X, y, max_samples=self.max_samples, **fit_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\repos\\BU\\BU-MSDS\\DX603\\DS_603_Team_17\\.conda\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 493, in _fit\n",
      "    raise ValueError(\"max_features must be <= n_features\")\n",
      "ValueError: max_features must be <= n_features\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\repos\\BU\\BU-MSDS\\DX603\\DS_603_Team_17\\.conda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-6.50779685e+10 -6.50260491e+10 -6.50297910e+10 -6.50192436e+10\n",
      " -6.50431573e+10 -6.50819963e+10 -6.50923529e+10 -7.23151990e+10\n",
      " -7.23916812e+10 -7.23819844e+10 -7.24987442e+10 -7.25511890e+10\n",
      " -7.24049622e+10 -7.23536991e+10             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 60 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "\n",
    "# suppress ConvergenceWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    for name, (model, param_grid) in models_gs.items():\n",
    "        grid_search = GridSearchCV(estimator=model, \n",
    "                            param_grid=param_grid, \n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            n_jobs=-1,\n",
    "                            cv=cv,\n",
    "                            verbose=1)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        scores = grid_search.best_score_\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        rmse_score = np.sqrt(-scores)              \n",
    "        rmse_table_gs.loc[name] = [rmse_score, best_params, 'original']\n",
    "\n",
    "print(\"Grid Search w/ Original Features Scores:\")\n",
    "print(rmse_table_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [4 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n",
    "- Provide a ranking of your three models and explain your reasoning — not just based on RMSE, but also interpretability, training time, or generalizability.\n",
    "- Conclude by considering whether this workflow has produced the results you expected. Typically, you would repeat steps 2 - 4 and also reconsider the choices you made in Milestone 1 when cleaning the dataset, until reaching the point of diminishing returns; do you think that would that have helped here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
